{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dennybritz/rnn-tutorial-rnnlm/blob/master/RNNLM.ipynb\n",
    "In this link there is RNN implementation by using numpy arrays only. \n",
    "\n",
    "As much as I understood this is implementing the ELman RNN without bias.\n",
    "Elman link: https://en.wikipedia.org/wiki/Recurrent_neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "vocabulary_size = 8000\n",
    "\n",
    "\n",
    "class RNNNumpy():\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
    "        # assign instance variable\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # random initiate the parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        xt = np.exp(x - np.max(x))\n",
    "        return xt / np.sum(xt)\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        # total num of time steps, 5 layers back in time maybe?\n",
    "        T = 5\n",
    "        # during forward propagation, save all hidden stages in s, S_t = U .dot x_t + W .dot s_{t-1}\n",
    "        # we also need the initial state of s, which is set to 0\n",
    "        # each time step is saved in one row in sï¼Œeach row in s is s[t] which corresponding to an rnn internal loop time\n",
    "        s = np.zeros((T+1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # output at each time step saved as o, save them for later use\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        for t in np.arange(T):\n",
    "            # we are indexing U by x[t]. it is the same as multiplying U with a one-hot vector\n",
    "#             print(self.U[:, int(x[t])])\n",
    "            s[t] = np.tanh(self.U[:, int(x[t])] + self.W.dot(s[t-1]))\n",
    "            o[t] = self.softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "\n",
    "fake_net = RNNNumpy(vocabulary_size, hidden_dim = 100, bptt_truncate = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example forward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 8000) (6, 100)\n"
     ]
    }
   ],
   "source": [
    "some_hot_encoded_input = np.zeros(vocabulary_size)\n",
    "some_hot_encoded_input[2] = 1\n",
    "result = fake_net.forward_propagation(some_hot_encoded_input)\n",
    "print(result[0].shape, result[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM implemantation forward only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y\n",
    "X_size = 1299\n",
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = (np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = (np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = (np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = (np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = (np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = (np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = (np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = (np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = (np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = (np.zeros((X_size, 1)))\n",
    "        \n",
    "    \n",
    "    def forward(self, x, h_prev, C_prev):\n",
    "        assert x.shape == (X_size, 1)\n",
    "        assert h_prev.shape == (H_size, 1)\n",
    "        assert C_prev.shape == (H_size, 1)\n",
    "\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        f = sigmoid(np.dot(self.W_f.v, z) + self.b_f.v)\n",
    "        i = sigmoid(np.dot(self.W_i.v, z) + self.b_i.v)\n",
    "        C_bar = tanh(np.dot(self.W_C.v, z) + self.b_C.v)\n",
    "\n",
    "        C = f * C_prev + i * C_bar\n",
    "        o = sigmoid(np.dot(self.W_o.v, z) + self.b_o.v)\n",
    "        h = o * tanh(C)\n",
    "\n",
    "        v = np.dot(self.W_v.v, h) + self.b_v.v\n",
    "        y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "        return z, f, i, C_bar, C, o, h, v, y\n",
    "    \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
