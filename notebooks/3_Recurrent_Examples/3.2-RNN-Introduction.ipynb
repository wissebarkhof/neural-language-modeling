{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "This is heavily influenced by [hedgehoglabs/xor](https://github.com/hedgehoglabs/xor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks (RNN)\n",
    "\n",
    "This notebook introduces the concept of Recurrent Neural Networks (RNN).\n",
    "You won't have as many programming tasks in this exercise, but do make sure that you read everyhing carefully, and understand what is going on.\n",
    "\n",
    "___\n",
    "\n",
    "A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.\n",
    "\n",
    "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
    "The idea is that the network should be able to use the previous computations as some form of memory and apply this to the future computation.\n",
    "An image may best explain how this is to be understood,\n",
    "\n",
    "![rnn-unroll image](../static_files/rnn-unfold.png)\n",
    "\n",
    "\n",
    "where it the network contains the following elements:\n",
    "\n",
    "- $x$ is the input sequence of samples, \n",
    "- $U$ is a weight matrix applied to the given input sample,\n",
    "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
    "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
    "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
    "- $o$ is the resulting output.\n",
    "\n",
    "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
    "We have the following computations through the network:\n",
    "\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ usually is an activation function, e.g. $\\mathrm{tanh}$.\n",
    "- $o_t = \\mathrm{softmax}(W\\,{h_t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Problem of Parity\n",
    "(parity simply means wheather the number is even or odd)\n",
    "\n",
    "The exercise comes from the OpenAI [Requests for Research 2.0](https://blog.openai.com/requests-for-research-2/) warmup exercise.\n",
    "The exercise reads:\n",
    "\n",
    "> Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:\n",
    "> 1. Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do you get?\n",
    "> 1. Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n",
    "\n",
    "LSTM stands for 'Long short-term memory', and is an extension of the standard RNN model described above.\n",
    "An LSTM has been extended with a memory module that enables it to remember over longer sequences.\n",
    "\n",
    "In this exercise we will however **stick with a standard RNN instead of an LSTM**.\n",
    "(The LSTM model will be covered later. See [this excelent blogpost](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah for more on LSTM's.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating the Data\n",
    "\n",
    "Before we do much of anything we need some data to work with.\n",
    "The code below we generate the binary vectors that we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils import rnn as rnn_utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "max_bits = 50\n",
    "batch_size = 8\n",
    "\n",
    "DEFAULT_NUM_BITS = 50\n",
    "DEFAULT_NUM_SEQUENCES = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions to load the data set\n",
    "\n",
    "class XORDataset(data.Dataset):\n",
    "    def __init__(self, num_sequences=DEFAULT_NUM_SEQUENCES, num_bits=DEFAULT_NUM_BITS):\n",
    "        self.num_sequences = num_sequences\n",
    "        self.num_bits = num_bits\n",
    "\n",
    "        self.features, self.labels = get_random_bits_parity(num_sequences, num_bits)\n",
    "\n",
    "        # expand the dimensions for the rnn\n",
    "        # [batch, bits] -> [batch, bits, 1]\n",
    "        self.features = np.expand_dims(self.features, -1)\n",
    "\n",
    "        # [batch, parity] -> [batch, parity, 1]\n",
    "        self.labels = np.expand_dims(self.labels, -1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index, :], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "\n",
    "def get_random_bits_parity(num_sequences, num_bits):\n",
    "    \"\"\"Generate random bit sequences and their parity. (Our features and labels).\n",
    "        Returns:\n",
    "            bit_sequences: A numpy array of bit sequences with shape [num_sequences, num_bits].\n",
    "            parity: A numpy array of even parity values corresponding to each bit\n",
    "                with shape [num_sequences, num_bits].\n",
    "        \"\"\"\n",
    "    bit_sequences = np.random.randint(2, size=(num_sequences, num_bits))\n",
    "\n",
    "    # if total number of ones is odd, set even parity bit to 1, otherwise 0\n",
    "    # https://en.wikipedia.org/wiki/Parity_bit\n",
    "\n",
    "#     bitsum = np.sum(bit_sequences, axis=1)  # use only the final result.\n",
    "    bitsum = np.cumsum(bit_sequences, axis=1)  # Teacher forcing\n",
    "\n",
    "    # if bitsum is even: False, odd: True\n",
    "    parity = bitsum % 2 != 0\n",
    "\n",
    "    return bit_sequences.astype('float32'), parity.astype('float32')\n",
    "\n",
    "train_loader = DataLoader(XORDataset(num_bits=max_bits), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what it looks like.\n",
    "In order to make the print statement look nicer we print the transpose of the data - i.e. the data is `[seq_len, num_seq]` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets  in train_loader:\n",
    "    print('inputs')\n",
    "    print(inputs.numpy()[:,:15].T)\n",
    "#     print(np.sum(inputs.numpy(), 1).T)\n",
    "    print()\n",
    "    print('targets')\n",
    "    print(targets.numpy()[:,:15].T)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than only have a target for the very last element in the sequence, we make a target for every sub-sequence along the way.\n",
    "During training we use all these targets to help the network learn along the way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Model\n",
    "\n",
    "One of the nice things with using a library like `PyTorch` is that a lot of the complexity is hidden away.\n",
    "Creating a RNN is therefore not much different than creating any other kind of network, as we shall see below.\n",
    "Do however make sure to understand the concepts - they will help you debug, or when you want to work with more advanced concepts.\n",
    "\n",
    "Things to note however:\n",
    " * `batch_first=True` determines the order of the sequence. Normally in `PyTorch` the input is expected to be `[seq_len, batch, input_size]`, but this is sometimes inconvenient, so we use `batch_first=True` to make the input `[batch, seq, feature]`. \n",
    " * `pack_padded_sequence` and `pad_packed_sequence` are used for variable length sequences when the sequences don't have the same length. (Computers don't deal well with sequences of varing length, so we use padding to make them align).\n",
    " \n",
    "For more on RNNs in `PyTorch` [see here](https://pytorch.org/docs/stable/nn.html#torch.nn.RNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = torch.nn.RNN(\n",
    "                batch_first=True,\n",
    "                input_size=1,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=1)\n",
    "\n",
    "        self.hidden_to_logits = torch.nn.Linear(hidden_size, 1)\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # pack the inputs\n",
    "        packed_inputs = rnn_utils.pack_padded_sequence(\n",
    "                inputs, lengths, batch_first=True).to(device)\n",
    "\n",
    "        rnn_out, _ = self.rnn(packed_inputs)\n",
    "\n",
    "        unpacked, _ = rnn_utils.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "\n",
    "        logits = self.hidden_to_logits(unpacked)\n",
    "        predictions = self.activation(logits)\n",
    "\n",
    "        return logits, predictions\n",
    "\n",
    "hidden_size = 4\n",
    "\n",
    "model = RNN(hidden_size, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As allways we need to define our optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "lr = 1\n",
    "momentum = 0\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions - you don't need to worry about them.\n",
    "\n",
    "def adjust_lengths(vary_lengths, inputs, targets):\n",
    "    batch_size = inputs.size()[0]\n",
    "    max_bits = inputs.size()[1]\n",
    "\n",
    "    if not vary_lengths:\n",
    "        lengths = torch.ones(batch_size, dtype=torch.int) * max_bits\n",
    "        return lengths\n",
    "\n",
    "    # choose random lengths\n",
    "    lengths = np.random.randint(1, max_bits, size=batch_size, dtype=int)\n",
    "\n",
    "    # keep one the max size so we don't need to resize targets for the loss\n",
    "    lengths[0] = max_bits\n",
    "\n",
    "    # sort in descending order\n",
    "    lengths = -np.sort(-lengths)\n",
    "\n",
    "    # chop the bits based on lengths\n",
    "    for i, sample_length in enumerate(lengths):\n",
    "        inputs[i, lengths[i]:, ] = 0\n",
    "        targets[i, lengths[i]:, ] = 0\n",
    "\n",
    "    return lengths\n",
    "\n",
    "\n",
    "def evaluate(max_bits, vary_lengths, device, model):\n",
    "    # evaluate on more bits than training to ensure generalization\n",
    "    valid_loader = DataLoader(\n",
    "            XORDataset(num_sequences=5000, num_bits=int(max_bits * 1.5)), batch_size=500)\n",
    "\n",
    "    is_correct = np.array([])\n",
    "\n",
    "    for inputs, targets in valid_loader:\n",
    "        lengths = adjust_lengths(vary_lengths, inputs, targets)\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, predictions = model(inputs, lengths)\n",
    "            is_correct = np.append(is_correct, ((predictions > 0.5) == (targets > 0.5)))\n",
    "\n",
    "    accuracy = is_correct.mean()\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "\n",
    "We are now ready to train the network. \n",
    "This is also very similar to what we have already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "epochs = 5\n",
    "vary_lengths = False\n",
    "\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for inputs, targets in train_loader:\n",
    "        lengths = adjust_lengths(vary_lengths, inputs, targets)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, predictions = model(inputs, lengths)\n",
    "\n",
    "        # BCEWithLogitsLoss will do the activation\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        accuracy = ((predictions > 0.5) == (targets > 0.5)).type(torch.FloatTensor).mean()\n",
    "        train_accs.append((step, accuracy))\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            #Note: If you receive an error at the following line, you are not using a Python 3.6.x kernel\n",
    "            print(f'epoch {epoch:2d}, step {step:5d}. Train summary: loss {loss.item():.{4}f}, accuracy {accuracy:.{3}f}')\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            valid_accuracy = evaluate(max_bits, vary_lengths, device, model)\n",
    "            valid_accs.append((step, valid_accuracy))\n",
    "            print(f'Validation accuracy {valid_accuracy:.{3}f}\\n')\n",
    "            if valid_accuracy == 1.0:\n",
    "                # stop early\n",
    "                print('Stop early: valid_accuracy == 1.0')\n",
    "                break\n",
    "    if valid_accuracy == 1.0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(train_accs)[:,0], np.array(train_accs)[:,1], label='Training accuracy')\n",
    "plt.plot(np.array(valid_accs)[:,0], np.array(valid_accs)[:,1], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    "1. The code provided uses fixed length sequences. Save your results, change `vary_lengths` to `True`, and run it again. Does the RNN still succeed? What explains the difference?\n",
    "1. Try and change the model architecture - e.g. number of units or number of layers. How does that affect performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Assignments\n",
    "1. Compare RNN and LSTM. Replace `torch.nn.RNN` with `torch.nn.LSTM` in order to use an LSTM instead. Compare the results of the LSTM and the RNN. Can you see a difference?\n",
    "1. Try and **remove teacher forcing**. This will require you to change the data generator `get_random_bits_parity` to use `np.sum` rather than `np.cumsum`. You also need to change some things in the training loop - use the error mesages to guide you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
