{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "my_tok = spacy.load('en')\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# we'll use the bokeh library to create beautiful plots\n",
    "# *_notebook functions are needed for correct use in jupyter\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook, show, push_notebook\n",
    "# output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data processing ----\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# improve using 'spacy'\n",
    "def tokenize(x):\n",
    "    return [tok.text for tok in my_tok.tokenizer(x)]\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "TEXT = data.Field(lower=True, tokenize=tokenize, sequential=True)\n",
    "\n",
    "# create splits\n",
    "train, valid, test = datasets.PennTreebank.splits(TEXT) # loading custom datasets requires passing in the field, but nothing else.\n",
    "\n",
    "# this takes a long long time (when you need to downloand the vectors (1st time)), but without a vocab I get errors\n",
    "TEXT.build_vocab(train, max_size=None, vectors=[GloVe(name='6B', dim='300')])\n",
    "\n",
    "# Create iterators of batch_size = 32\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    bptt_len=30, # this is where we specify the sequence length\n",
    "    device=torch.device('cpu'),\n",
    "    repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 300\n",
      "Num Embeddings: 9731\n"
     ]
    }
   ],
   "source": [
    "# size of embeddings\n",
    "embedding_dim = TEXT.vocab.vectors.size()[1]\n",
    "num_embeddings = TEXT.vocab.vectors.size()[0]\n",
    "\n",
    "print('Embedding dim: {}'.format(embedding_dim))\n",
    "print('Num Embeddings: {}'.format(num_embeddings))\n",
    "\n",
    "# copied from http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    " \n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5, tie_weights=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz) # the input is a batched consecutive corpus\n",
    "                                            # therefore, we retain the hidden state across batches\n",
    " \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    " \n",
    "    def forward(self, input):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    " \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (V(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                V(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "  \n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(V(v.data) for v in self.hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "        ...,\n",
       "        [ 0.3020,  0.6570,  0.4714,  ...,  0.5794,  0.2269, -0.1504],\n",
       "        [-0.0879,  0.0287,  0.1615,  ..., -0.3171, -0.2749, -0.3325],\n",
       "        [-0.0446,  0.2063,  0.0142,  ..., -0.1955, -0.2697,  0.6638]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model with the pretrained vectors\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = RNNModel(weight_matrix.size(0), weight_matrix.size(1), 200, 1, BATCH_SIZE)\n",
    "\n",
    "\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.7, 0.99))\n",
    "n_tokens = weight_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm import tqdm \n",
    "def train_epoch(epoch):\n",
    "    \"\"\"One epoch of a training loop\"\"\"\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_iter):\n",
    "    # reset the hidden state or else the model will try to backpropagate to the\n",
    "    # beginning of the dataset, requiring lots of time and a lot of memory\n",
    "         model.reset_history()\n",
    " \n",
    "    optimizer.zero_grad()\n",
    " \n",
    "    text, targets = batch.text, batch.target\n",
    "    prediction = model(text)\n",
    "    # pytorch currently only supports cross entropy loss for inputs of 2 or 4 dimensions.\n",
    "    # we therefore flatten the predictions out across the batch axis so that it becomes\n",
    "    # shape (batch_size * sequence_length, n_tokens)\n",
    "    # in accordance to this, we reshape the targets to be\n",
    "    # shape (batch_size * sequence_length)\n",
    "    loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "    loss.backward()\n",
    " \n",
    "    optimizer.step()\n",
    " \n",
    "    epoch_loss += loss.data[0] * prediction.size(0) * prediction.size(1)\n",
    " \n",
    "    epoch_loss /= len(train.examples[0].text)\n",
    " \n",
    "    # monitor the loss\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    for batch in valid_iter:\n",
    "        model.reset_history()\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        val_loss += loss.data[0] * text.size(0)\n",
    "    val_loss /= len(valid.examples[0].text)\n",
    " \n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 5599.53it/s]\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 6042.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.0016, Validation Loss: 0.2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.0016, Validation Loss: 0.2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 4338.04it/s]\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 6210.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.0016, Validation Loss: 0.2816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 5981.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.0016, Validation Loss: 0.2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 5803.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.0015, Validation Loss: 0.2757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 5735.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.0015, Validation Loss: 0.2693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/1131 [00:00<03:06,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.0014, Validation Loss: 0.2592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 5732.47it/s]\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 5932.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.0013, Validation Loss: 0.2496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 6002.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.0012, Validation Loss: 0.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 6013.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.0011, Validation Loss: 0.2372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 5913.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training Loss: 0.0010, Validation Loss: 0.2338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 5926.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training Loss: 0.0009, Validation Loss: 0.2329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1131/1131 [00:00<00:00, 5997.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training Loss: 0.0009, Validation Loss: 0.2334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/1131 [00:00<03:39,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training Loss: 0.0008, Validation Loss: 0.2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 5058.30it/s]\n",
      "  0%|          | 1/1131 [00:00<03:33,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Training Loss: 0.0008, Validation Loss: 0.2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1131/1131 [00:00<00:00, 5159.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "n_epochs = 50\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
